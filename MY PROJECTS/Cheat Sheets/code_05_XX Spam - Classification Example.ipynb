{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c593e910",
   "metadata": {},
   "source": [
    "### 5.1. Setup\n",
    "\n",
    "Install required text processing libraries for the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d8262a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/linkedin/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages (3.6.3)\n",
      "Requirement already satisfied: tqdm in /Users/linkedin/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: regex in /Users/linkedin/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from nltk) (2021.9.30)\n",
      "Requirement already satisfied: click in /Users/linkedin/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in /Users/linkedin/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from nltk) (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/linkedin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/linkedin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/linkedin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c3a4f",
   "metadata": {},
   "source": [
    "### 5.2. Creating Text Representations\n",
    "\n",
    "Text data needs to be converted to numeric representations before they can be used to train deep learning models. The Spam classification feature data is converted to TF-IDF vectors and the target variable is converted to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08900f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded Data :\n",
      "------------------------------------\n",
      "  CLASS                                                SMS\n",
      "0   ham   said kiss, kiss, i can't do the sound effects...\n",
      "1   ham      &lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\n",
      "2  spam  (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
      "3  spam  * FREE* POLYPHONIC RINGTONE Text SUPER to 8713...\n",
      "4  spam  **FREE MESSAGE**Thanks for using the Auction S...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "#Load Spam Data and review content\n",
    "spam_data = pd.read_csv(\"Spam-Classification.csv\")\n",
    "\n",
    "print(\"\\nLoaded Data :\\n------------------------------------\")\n",
    "print(spam_data.head())\n",
    "\n",
    "#Separate feature and target data\n",
    "spam_classes_raw = spam_data[\"CLASS\"]\n",
    "spam_messages = spam_data[\"SMS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64202dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape :  (1500, 4566)\n",
      "One-hot Encoding Shape :  (1500, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "\n",
    "#Custom tokenizer to remove stopwords and use lemmatization\n",
    "def customtokenize(str):\n",
    "    #Split string as tokens\n",
    "    tokens=nltk.word_tokenize(str)\n",
    "    #Filter for stopwords\n",
    "    nostop = list(filter(lambda token: token not in stopwords.words('english'), tokens))\n",
    "    #Perform lemmatization\n",
    "    lemmatized=[lemmatizer.lemmatize(word) for word in nostop ]\n",
    "    return lemmatized\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Build a TF-IDF Vectorizer model\n",
    "vectorizer = TfidfVectorizer(tokenizer=customtokenize)\n",
    "\n",
    "#Transform feature input to TF-IDF\n",
    "tfidf=vectorizer.fit_transform(spam_messages)\n",
    "#Convert TF-IDF to numpy array\n",
    "tfidf_array = tfidf.toarray()\n",
    "\n",
    "#Build a label encoder for target variable to convert strings to numeric values.\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "spam_classes = label_encoder.fit_transform(\n",
    "                                spam_classes_raw)\n",
    "\n",
    "#Convert target to one-hot encoding vector\n",
    "spam_classes = tf.keras.utils.to_categorical(spam_classes,2)\n",
    "\n",
    "print(\"TF-IDF Matrix Shape : \", tfidf.shape)\n",
    "print(\"One-hot Encoding Shape : \", spam_classes.shape)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split( tfidf_array, spam_classes, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a983f",
   "metadata": {},
   "source": [
    "### 5.3. Building and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d927db5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Hidden-Layer-1 (Dense)       (None, 32)                146144    \n",
      "_________________________________________________________________\n",
      "Hidden-Layer-2 (Dense)       (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "Output-Layer (Dense)         (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 147,266\n",
      "Trainable params: 147,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "#Setup Hyper Parameters for building the model\n",
    "NB_CLASSES=2\n",
    "N_HIDDEN=32\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                             input_shape=(X_train.shape[1],),\n",
    "                              name='Hidden-Layer-1',\n",
    "                              activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                              name='Hidden-Layer-2',\n",
    "                              activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "                             name='Output-Layer',\n",
    "                             activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "294ceb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Progress:\n",
      "------------------------------------\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 37ms/step - loss: 0.6866 - accuracy: 0.6370 - val_loss: 0.6652 - val_accuracy: 0.8741\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6388 - accuracy: 0.9574 - val_loss: 0.6146 - val_accuracy: 0.9370\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5743 - accuracy: 0.9796 - val_loss: 0.5619 - val_accuracy: 0.9407\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5082 - accuracy: 0.9843 - val_loss: 0.5100 - val_accuracy: 0.9593\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4445 - accuracy: 0.9889 - val_loss: 0.4620 - val_accuracy: 0.9519\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3836 - accuracy: 0.9907 - val_loss: 0.4137 - val_accuracy: 0.9630\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3270 - accuracy: 0.9917 - val_loss: 0.3705 - val_accuracy: 0.9630\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2756 - accuracy: 0.9917 - val_loss: 0.3304 - val_accuracy: 0.9630\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2299 - accuracy: 0.9926 - val_loss: 0.2952 - val_accuracy: 0.9630\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1903 - accuracy: 0.9944 - val_loss: 0.2637 - val_accuracy: 0.9556\n",
      "\n",
      "Accuracy during Training :\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAE/CAYAAACJnoCmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAq4klEQVR4nO3dfXhcd33n/fdHT5Zly5YTO44tO7ZDbBIndhLqhrbZQO5CINBAKN1tkwIbelNyc21DWR62C902sGG5272329LdZummbUopLWkauqxJTQMlBMoCrZ0Sj2wHJ44h8YwfosTW+Em2nr73H+f
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c593e910",
   "metadata": {},
   "source": [
    "### 5.1. Setup\n",
    "\n",
    "Install required text processing libraries for the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d8262a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/linkedin/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages (3.6.3)\n",
      "Requirement already satisfied: tqdm in /Users/linkedin/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: regex in /Users/linkedin/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from nltk) (2021.9.30)\n",
      "Requirement already satisfied: click in /Users/linkedin/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in /Users/linkedin/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from nltk) (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/linkedin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/linkedin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/linkedin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c3a4f",
   "metadata": {},
   "source": [
    "### 5.2. Creating Text Representations\n",
    "\n",
    "Text data needs to be converted to numeric representations before they can be used to train deep learning models. The Spam classification feature data is converted to TF-IDF vectors and the target variable is converted to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08900f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded Data :\n",
      "------------------------------------\n",
      "  CLASS                                                SMS\n",
      "0   ham   said kiss, kiss, i can't do the sound effects...\n",
      "1   ham      &lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\n",
      "2  spam  (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
      "3  spam  * FREE* POLYPHONIC RINGTONE Text SUPER to 8713...\n",
      "4  spam  **FREE MESSAGE**Thanks for using the Auction S...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "#Load Spam Data and review content\n",
    "spam_data = pd.read_csv(\"Spam-Classification.csv\")\n",
    "\n",
    "print(\"\\nLoaded Data :\\n------------------------------------\")\n",
    "print(spam_data.head())\n",
    "\n",
    "#Separate feature and target data\n",
    "spam_classes_raw = spam_data[\"CLASS\"]\n",
    "spam_messages = spam_data[\"SMS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64202dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape :  (1500, 4566)\n",
      "One-hot Encoding Shape :  (1500, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "\n",
    "#Custom tokenizer to remove stopwords and use lemmatization\n",
    "def customtokenize(str):\n",
    "    #Split string as tokens\n",
    "    tokens=nltk.word_tokenize(str)\n",
    "    #Filter for stopwords\n",
    "    nostop = list(filter(lambda token: token not in stopwords.words('english'), tokens))\n",
    "    #Perform lemmatization\n",
    "    lemmati